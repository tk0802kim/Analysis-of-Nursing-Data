{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "biblical-wilderness",
   "metadata": {},
   "source": [
    "# Prediction Models\n",
    "\n",
    "This notebook is the refined prediction model using sklearn pipelines and xgboost cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "missing-presentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PN_CS_PUF_6.0 and PN_LVL_PUF_7.0 have 0.9005905917050862 correlation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import pyreadstat\n",
    "import re\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import auc\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "# %% cell 1\n",
    "# load the state abbreviation and population stuff. Pickeld by state_name\n",
    "with open(\"pickles\\\\state_census.pkl\", \"rb\") as f:\n",
    "    state_census = pickle.load(f)\n",
    "\n",
    "region = {\n",
    "    'D1': ['VT', 'RI'],\n",
    "    'D4': ['ND', 'SD'],\n",
    "    'D8': ['MT', 'WY'],\n",
    "    'D9': ['HI', 'AK']\n",
    "}\n",
    "\n",
    "# function to handle region to state, and vice versa\n",
    "\n",
    "\n",
    "def r2s(dict_in):\n",
    "    dict_out = copy.deepcopy(dict_in)\n",
    "    for key in dict_in.keys():\n",
    "        if key in region:\n",
    "            for st in region[key]:\n",
    "                dict_out[st] = dict_in[key]\n",
    "            dict_out.pop(key)\n",
    "    return dict_out\n",
    "\n",
    "\n",
    "def s2r(dict_in):\n",
    "    dict_out = copy.deepcopy(dict_in)\n",
    "    for key in region:\n",
    "        dict_out[key] = np.mean([dict_in[st] for st in region[key]])\n",
    "        for st in region[key]:\n",
    "            dict_out.pop(st)\n",
    "    return dict_out\n",
    "\n",
    "# function that takes region, and gives back state names\n",
    "\n",
    "\n",
    "def r2slist(st):\n",
    "    if st in region:\n",
    "        return region[st]\n",
    "    else:\n",
    "        return [st]\n",
    "\n",
    "\n",
    "def slist2r(st):\n",
    "    for key in region:\n",
    "        if st in region[key]:\n",
    "            return key\n",
    "\n",
    "\n",
    "def region_explain():\n",
    "    print(' ')\n",
    "    print('(D1: VT, RI     D2: ND,SD      D8: MT, WY     D9: HI, AK)')\n",
    "\n",
    "\n",
    "df_2018, meta_2018 = pyreadstat.read_sas7bdat(\n",
    "    'NSSRN2018_SAS_encoded_package\\\\NSSRN_2018_PUF.sas7bdat')\n",
    "weight = df_2018['RKRNWGTA']\n",
    "\n",
    "# explore the D's to see if they should be combined\n",
    "titles = ['Graduated State', 'First Licensed State', 'Current State']\n",
    "varnames = ['ED_NDLOC_ST_PUF', 'ED_FRN_ST_PUF', 'PN_LOC_ST_PUF']\n",
    "\n",
    "\n",
    "# combine states into regions\n",
    "for key in region.keys():\n",
    "    for st in region[key]:\n",
    "        for vn in varnames:\n",
    "            df_2018.loc[df_2018[vn] == st, vn] = key\n",
    "\n",
    "\n",
    "def explain(name):\n",
    "\n",
    "    underscore = -name[::-1].find('_')\n",
    "\n",
    "    if name[underscore:].isnumeric():\n",
    "        code = name[:underscore-1]\n",
    "        num = int(name[underscore:])\n",
    "    else:\n",
    "        code = name\n",
    "        num = 0\n",
    "\n",
    "    try:\n",
    "        info = survey_key.loc[survey_key['2018 NSSRN VARIABLE NAME'] == code, [\n",
    "            '2018 NSSRN QUESTIONNAIRE ITEM NUMBER', 'DESCRIPTION']].to_numpy()[0]\n",
    "        print('{}: {}'.format(info[0], info[1]))\n",
    "\n",
    "        if num != 0:\n",
    "            text = survey_key['VALUES'][survey_key['2018 NSSRN VARIABLE NAME'] == code].to_numpy()[\n",
    "                0]\n",
    "            ind = [m.start() for m in re.finditer('\\n', text)]\n",
    "            ind.insert(0, -1)\n",
    "            ind.append(len(text))\n",
    "            print('    '+text[ind[num-1]+1:ind[num]])\n",
    "    except:\n",
    "        print('Does not exist')\n",
    "\n",
    "\n",
    "survey_key = pd.read_excel('survey_key.xls').replace(np.nan, '')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "try a few different approaches:\n",
    "    only PN_5YRS==1 vs all\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# df_learn = copy.deepcopy(df_2018).loc[(df_2018['PN_EMPLYD']==1)&(df_2018['PN_NEWEMP']==1)&\n",
    "#                                       (df_2018['PN_PATCARE']==1)&(df_2018['PN_EHR']!=3),:]\n",
    "df_learn = copy.deepcopy(df_2018).loc[(df_2018['PN_EMPLYD'] == 1) & (df_2018['PN_5YRS'] == 1) &\n",
    "                                      (df_2018['PN_PATCARE'] == 1) & (df_2018['PN_EHR'] != 3), :]\n",
    "\n",
    "leave_reasons = survey_key['2018 NSSRN VARIABLE NAME'][survey_key['2018 NSSRN QUESTIONNAIRE ITEM NUMBER'] == 'C1'].to_numpy()\n",
    "outside_reasons = ['LE_LVE_DISAB', 'LE_LVE_FAM', 'LE_LVE_LAID',\n",
    "                   'LE_LVE_COMMTE', 'LE_LVE_RETIRE', 'LE_LVE_SPEMP']\n",
    "#outside_reasons=['LE_LVE_DISAB', 'LE_LVE_LAID']\n",
    "inside_reasons = []\n",
    "for reason in leave_reasons:\n",
    "    if reason not in outside_reasons:\n",
    "        inside_reasons.append(reason)\n",
    "\n",
    "any_outside = np.zeros(df_learn.shape[0])\n",
    "for reason in outside_reasons:\n",
    "    any_outside = np.logical_or(\n",
    "        any_outside, (df_learn[reason] == 1).to_numpy())\n",
    "\n",
    "\n",
    "df_learn = df_learn.loc[~any_outside, :]\n",
    "\n",
    "\n",
    "\"\"\"droplist\"\"\"\n",
    "# populate dropcols we are gonna use\n",
    "dropcols = []\n",
    "#keep = ['B','C','D','E','F','H']\n",
    "keep = ['B']\n",
    "remove = ['B2']\n",
    "\n",
    "# make drop list of unnecessary stuff\n",
    "for i, row in survey_key.iterrows():\n",
    "\n",
    "    if row['2018 NSSRN QUESTIONNAIRE ITEM NUMBER'] == '':\n",
    "        dropcols.append(row['2018 NSSRN VARIABLE NAME'])\n",
    "\n",
    "    elif ((row['2018 NSSRN QUESTIONNAIRE ITEM NUMBER'] == 'IMP_FLAG') or\n",
    "          (row['2018 NSSRN QUESTIONNAIRE ITEM NUMBER'] == 'DERIVED') or\n",
    "          not(row['2018 NSSRN QUESTIONNAIRE ITEM NUMBER'][0] in keep) or\n",
    "          (row['Action1'] == 'del') or\n",
    "          (row['2018 NSSRN QUESTIONNAIRE ITEM NUMBER'] in remove)):\n",
    "        dropcols.append(row['2018 NSSRN VARIABLE NAME'].strip())\n",
    "\n",
    "# drop stuff in dropcols\n",
    "df_learn.drop(dropcols, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"bool list\"\"\"\n",
    "# find boolean variables\n",
    "boolcols = []\n",
    "\n",
    "for i, row in survey_key.iterrows():\n",
    "    if row['VALUES'].strip() == '1 =\"YES\"\\n2 =\"NO\"':\n",
    "        boolcols.append(row['2018 NSSRN VARIABLE NAME'].strip())\n",
    "\n",
    "# revalue false from 2 to 0\n",
    "for col in boolcols:\n",
    "    if col in df_learn.columns:\n",
    "        df_learn.loc[df_learn[col] == 2, col] = 0\n",
    "\n",
    "\n",
    "# %% pre-split processing\n",
    "y = df_learn['PN_LFTWRK']\n",
    "X = df_learn.drop(columns=['PN_LFTWRK'])\n",
    "\n",
    "# remapping\n",
    "X.loc[X['PN_NEWEMP'] == 1, 'PN_NEWEMP'] = 3/12\n",
    "X.loc[X['PN_NEWEMP'] == 2, 'PN_NEWEMP'] = 9/12\n",
    "X.loc[X['PN_NEWEMP'] == 3, 'PN_NEWEMP'] = 1\n",
    "X.loc[X['PN_NEWEMP'] == 4, 'PN_NEWEMP'] = 0\n",
    "\n",
    "X.loc[(X[\"PN_ORIENT\"] == 1) & (X[\"PN_PRECEP\"] == 0), \"PN_ORIENT\"] = 0.5\n",
    "\n",
    "X.loc[X['PN_EHR'] == 2, 'PN_EHR'] = 0\n",
    "X.loc[X['PN_EHR'] == 3, 'PN_EHR'] = 0.5\n",
    "\n",
    "for name in ['PN_WE_TBC', 'PN_WE_IPT', 'PN_WE_HIT', 'PN_OE_CARE', 'PN_OE_DISPL', 'PN_OE_TBC', 'PN_OE_EBC']:\n",
    "    X.loc[X[name] == 2, name] = 2/3\n",
    "    X.loc[X[name] == 3, name] = 1/3\n",
    "    X.loc[X[name] == 4, name] = 0\n",
    "    X.loc[X[name] == 5, name] = 0\n",
    "\n",
    "for name in ['PN_THTYP_PTP', 'PN_THTYP_RN', 'PN_THTYP_NP', 'PN_THTYP_OTH_PUF', 'PN_THTYP_PVDTP']:\n",
    "    X.loc[X[name].isna(), name] = 0\n",
    "\n",
    "X.loc[X['PN_SATISFD'] == 2, 'PN_SATISFD'] = 2/3\n",
    "X.loc[X['PN_SATISFD'] == 3, 'PN_SATISFD'] = 1/3\n",
    "X.loc[X['PN_SATISFD'] == 4, 'PN_SATISFD'] = 0\n",
    "\n",
    "X['PN_WRK'] += -1\n",
    "X['PN_MTHPY'] = X['PN_MTHPY']/12\n",
    "X[['PN_HRS_SCHED_PUF', 'PN_HRS_WRK_PUF']] = X[[\n",
    "    'PN_HRS_SCHED_PUF', 'PN_HRS_WRK_PUF']]/40\n",
    "X[['PN_TS_PCC', 'PN_TS_CARE', 'PN_TS_SUPER', 'PN_TS_RESRCH', 'PN_TS_TEACH', 'PN_TS_NNT', 'PN_TS_OTH']] = X[[\n",
    "    'PN_TS_PCC', 'PN_TS_CARE', 'PN_TS_SUPER', 'PN_TS_RESRCH', 'PN_TS_TEACH', 'PN_TS_NNT', 'PN_TS_OTH']]/100\n",
    "X[['PN_POP_PNAT', 'PN_POP_NEWB', 'PN_POP_PED', 'PN_POP_ADOL', 'PN_POP_ADLT', 'PN_POP_GER']] = X[[\n",
    "    'PN_POP_PNAT', 'PN_POP_NEWB', 'PN_POP_PED', 'PN_POP_ADOL', 'PN_POP_ADLT', 'PN_POP_GER']]/100\n",
    "\n",
    "# addtional drops\n",
    "# PN_PRECEP: merged to PN_ORIENT\n",
    "# PN_TELHLTH, PN_THPERS,\tPN_THTYP: dont care about telehealth\n",
    "\n",
    "dropcols2 = ['PN_PRECEP', 'PN_TELHLTH', 'PN_THPERS', 'PN_THTYP_PTP', 'PN_THTYP_RN', 'PN_THTYP_NP',\n",
    "             'PN_THTYP_OTH_PUF', 'PN_THTYP_PVDTP']  # ,'PN_TS_PCC','PN_POP_PNAT']\n",
    "X.drop(dropcols2, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# check if theres any null\n",
    "X.isnull().values.any()\n",
    "\n",
    "# categorical encoders\n",
    "cat_cols = ['PN_EMPSIT', 'PN_EMPSET', 'PN_LVL_PUF', 'PN_CS_PUF']\n",
    "cat_drop_vals = []\n",
    "\n",
    "for name in cat_cols:\n",
    "    cat_drop_vals.append(name+'_'+str(X[name].value_counts().index[0]))\n",
    "\n",
    "X = pd.get_dummies(X, columns=cat_cols)\n",
    "\n",
    "#check correlation\n",
    "drop_corr =[]\n",
    "cor = X.corr()\n",
    "for i, ind in enumerate(X.columns):\n",
    "    for j, jnd in enumerate(X.columns):\n",
    "        if i > j and cor.iloc[i, j] > 0.9:\n",
    "            print('{} and {} have {} correlation'.format(ind, jnd, cor.iloc[i, j]))\n",
    "            drop_corr.append(random.choice([ind,jnd]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-cattle",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "After encoding features, we find two columns with high correlation. In linear models, one will be randomly chosen to be dropped, in trees, both will be kept.\n",
    "\n",
    "Two oversampling methods, SMOTE and random oversampling, were used in addtion to the base data. Metric used to evaluate models are AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "right-supervision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none: 0.759682226040009\n",
      "RO: 0.7560028746960237\n",
      "SMOTE: 0.7557197852738343\n"
     ]
    }
   ],
   "source": [
    "# %% post-split processing and logistic prediction\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.drop(columns=cat_drop_vals+drop_corr), y, test_size=0.2, random_state=2021)\n",
    "\n",
    "# pipelines\n",
    "scale_cols = ['PN_MTHPY', 'PN_HRS_SCHED_PUF', 'PN_HRS_WRK_PUF', 'PN_TS_PCC', 'PN_TS_CARE',\n",
    "              'PN_TS_SUPER', 'PN_TS_RESRCH', 'PN_TS_TEACH', 'PN_TS_NNT', 'PN_TS_OTH', 'PN_POP_PNAT',\n",
    "              'PN_POP_NEWB', 'PN_POP_PED', 'PN_POP_ADOL', 'PN_POP_ADLT', 'PN_POP_GER', 'PN_EARN_PUF']\n",
    "\n",
    "lr_preprocess = ColumnTransformer([('scaler', RobustScaler(), scale_cols)\n",
    "                                   # ,('dummy',OneHotEncoder(drop=cat_drop_vals),cat_cols)\n",
    "                                   ], remainder='passthrough')\n",
    "\n",
    "\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('prep', lr_preprocess),\n",
    "    ('logistic', LogisticRegression(penalty='l2', max_iter=1000))\n",
    "])\n",
    "\n",
    "lr_RO_pipeline = Pipeline(steps=[\n",
    "    ('prep', lr_preprocess),\n",
    "    ('OS', RandomOverSampler()),\n",
    "    ('logistic', LogisticRegression(penalty='l2', max_iter=1000))\n",
    "])\n",
    "\n",
    "lr_SMOTE_pipeline = Pipeline(steps=[\n",
    "    ('prep', lr_preprocess),\n",
    "    ('OS', SMOTE()),\n",
    "    ('logistic', LogisticRegression(penalty='l2', max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "models = {'none': GridSearchCV(estimator=lr_pipeline, param_grid={'logistic__C': np.logspace(-4, 0.3, 10)}, cv=5, scoring='roc_auc'),\n",
    "          'RO': GridSearchCV(estimator=lr_RO_pipeline, param_grid={'logistic__C': np.logspace(-4, 0.3, 10)}, cv=5, scoring='roc_auc'),\n",
    "          'SMOTE': GridSearchCV(estimator=lr_SMOTE_pipeline, param_grid={'logistic__C': np.logspace(-4, 0.3, 10)}, cv=5, scoring='roc_auc'),\n",
    "          }\n",
    "\n",
    "results = {}\n",
    "\n",
    "for key in models:\n",
    "    models[key].fit(X_train, y_train)\n",
    "    results[key] = pd.DataFrame(data=models[key].cv_results_)\n",
    "\n",
    "for key in models:\n",
    "    print('{}: {}'.format(key, models[key].score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-corps",
   "metadata": {},
   "source": [
    "No over sampling does better than oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-halifax",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "No oversampling was used. Pipelines from sklearn were used, but the crossvalidation was done with cv in xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "leading-place",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7673447023343116"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=2021)\n",
    "\n",
    "xgb_preprocess = ColumnTransformer([('scaler', RobustScaler(), scale_cols)\n",
    "                                    # ,('onehot',OneHotEncoder(),cat_cols)\n",
    "                                    ], remainder='passthrough')\n",
    "\n",
    "\n",
    "xgb_X_train = xgb_preprocess.fit_transform(X_train)\n",
    "xgb_X_test = xgb_preprocess.transform(X_test)\n",
    "\n",
    "dtrain = xgb.DMatrix(xgb_X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(xgb_X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'max_depth': 2,\n",
    "    'learning_rate': 0.1,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 4,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'eval_metric': 'auc',\n",
    "    'objective': 'binary:logistic'}\n",
    "\n",
    "\"\"\" following snippet was used to try different parameters to tune the best param\"\"\"\n",
    "# for i in range(2, 10):\n",
    "#     param['min_child_weight'] = i\n",
    "#     xgb_results = xgb.cv(param, dtrain, num_boost_round=200,\n",
    "#                          early_stopping_rounds=10, nfold=5)\n",
    "#     print('{}: {}'.format(i, xgb_results['test-auc-mean'].values[-1]))\n",
    "\n",
    "\n",
    "xgb_model = xgb.train(param, dtrain, num_boost_round=200)\n",
    "metrics.roc_auc_score(y_test, xgb_model.predict(dtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-estate",
   "metadata": {},
   "source": [
    "It performs slightly better than LR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
